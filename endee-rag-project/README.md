# Endee RAG Knowledge Assistant

**Author:** Syed Zeeyan  
**Project Type:** Retrieval Augmented Generation (RAG) using Endee Vector Database  
**Purpose:** Endee Labs Internship Evaluation

## ğŸ” Project Overview

This project demonstrates a **complete Retrieval Augmented Generation (RAG) pipeline** built using the **Endee vector database** as the core semantic search engine.

The system ingests documents, converts them into embeddings, stores them in Endee, and retrieves relevant context for user queries using vector similarity search.

This implementation is designed to be **fully local**, **cost-free**, and **production-structured**, showcasing practical understanding of vector databases and RAG architecture.

## ğŸš€ Problem Statement

Traditional search systems rely on keyword matching and often fail to understand semantic meaning.

**Modern AI systems require:**
- Semantic search over knowledge bases
- Context retrieval for intelligent responses
- Fast vector similarity search
- Local/private data processing

This project solves these challenges by implementing a complete RAG pipeline using **Endee**.

## âœ¨ Key Features

- âœ… Uses **Endee** as the core vector database
- âœ… Implements **complete RAG pipeline**
- âœ… Fully local (**no paid APIs required**)
- âœ… Document ingestion + semantic search
- âœ… Clean modular Python backend
- âœ… Docker-based Endee deployment
- âœ… GitHub-ready project structure

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    User([User Query]) -->|Embedding| QueryEmb[Query Embedding (MiniLM)]
    QueryEmb -->|Search| Endee[(Endee Vector Database)]
    Endee -->|Semantic Similarity| TopK[Top-K Relevant Chunks]
    TopK -->|Use Context| Response[Context-based Response]
```

## ğŸ› ï¸ How Endee is Used

Endee is the core engine of this project.

### 1. Index Creation
Creates a vector index for storing embeddings.

```http
POST /api/v1/index/create
{
  "index_name": "knowledge_base",
  "dim": 384,
  "space_type": "cosine"
}
```

### 2. Vector Storage
Document chunks are converted into embeddings and stored.

```http
POST /api/v1/index/{index_name}/vector/insert
```

Each vector stores:
- **embedding** (vector array)
- **meta** including text source 

### 3. Semantic Search
User query â†’ embedding â†’ vector search:

```http
POST /api/v1/index/{index_name}/search
{
  "vector": [...],
  "k": 3
}
```
Returns most relevant document chunks.

## ğŸ’» Tech Stack

| Component | Technology |
|---|---|
| **Vector Database** | Endee |
| **Backend** | Python |
| **Embeddings** | sentence-transformers |
| **Model** | all-MiniLM-L6-v2 |
| **Container** | Docker |
| **Interface** | CLI |

## ğŸ“‚ Project Structure

```
endee-rag-app/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/          # Document loading & chunking
â”‚   â”œâ”€â”€ embeddings/         # Embedding generation
â”‚   â”œâ”€â”€ retrieval/          # Query engine (RAG)
â”‚   â”œâ”€â”€ endee/              # Endee API client
â”‚   â””â”€â”€ main.py             # CLI entry point
â”‚
â”œâ”€â”€ data/documents/         # Knowledge base files
â”œâ”€â”€ docker-compose.yml      # Endee setup (if applicable)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš¡ Setup Instructions

### 1. Clone Repository
```bash
git clone https://github.com/YOUR_USERNAME/endee-rag-app.git
cd endee-rag-app
```

### 2. Create Virtual Environment
```bash
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
# source venv/bin/activate

pip install -r requirements.txt
```

### 3. Start Endee
Ensure you have Endee running (e.g., via Docker):
```bash
docker-compose up -d
```
Verify it's running:
```bash
curl http://localhost:8080/health
```

## â–¶ï¸ Running the Project

### Ingest Documents
Place your files inside `data/documents/`:
```bash
python -m src.main ingest data/documents
```

### Query System
Ask semantic questions about your documents:
```bash
python -m src.main query "What is artificial intelligence?"
```

> **Note:** Responses are generated from retrieved document context. Quality depends on documents ingested into the system.

## ğŸŒŸ Why This Project Matters

This project demonstrates:
- Practical use of **vector databases**
- Real-world **RAG implementation**
- **API integration** and debugging
- **System design** understanding
- Clean **modular backend** development

It shows the ability to build production-style AI systems using open-source tools.

## ğŸ”® Future Improvements

- [ ] Web UI (FastAPI + React)
- [ ] Hybrid search support
- [ ] Streaming responses
- [ ] Multi-document ranking
- [ ] Local LLM integration

## ğŸ‘¨â€ğŸ’» Author

**Syed Zeeyan**  
Backend & AI Engineering Candidate

This project was built as part of the **Endee Labs internship evaluation** to demonstrate real-world vector database and RAG system implementation.
